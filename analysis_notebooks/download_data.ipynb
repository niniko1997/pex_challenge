{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Create labeled image dataset with two classes\n",
    "\n",
    "1. Indoor photographs (e.g. Bedrooms, Bathrooms, Classrooms, Offices) \n",
    "2. Outdoor photographs (e.g. Landscapes, Skyscrapers, Mountains, Beaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import sklearn\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/administrator/Documents/pex_challenge/data/yt8m/frame'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# project directory\n",
    "project_dir = Path('/Users/administrator/Documents/pex_challenge/')\n",
    "data_dir = project_dir.joinpath('data/yt8m/frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Download a subset of examples from the YouTube-8M labeled video dataset: https://research.google.com/youtube8m/explore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/administrator/Documents/pex_challenge/data/yt8m/frame\n"
     ]
    }
   ],
   "source": [
    "# change directories into data_dir, where we want to download the data\n",
    "%cd {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading http://us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 8.0%rain/trainZ6.tfrecord 4.5%us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 4.7%rain/trainZ6.tfrecord 4.8%us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 4.9%e/train/trainZ6.tfrecord 5.0%rain/trainZ6.tfrecord 5.1%.1%://us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 5.2%rame/train/trainZ6.tfrecord 5.3%cord 5.4%us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 5.5%e/train/trainZ6.tfrecord 5.5%d 5.6%rain/trainZ6.tfrecord 5.7%.8%://us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 5.8%rame/train/trainZ6.tfrecord 5.9%cord 6.0%g http://us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 6.1%us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 6.2%e/train/trainZ6.tfrecord 6.3%rain/trainZ6.tfrecord 6.4%rain/trainZ6.tfrecord 6.5%rain/trainZ6.tfrecord 6.5%rain/trainZ6.tfrecord 6.6%rain/trainZ6.tfrecord 6.7%.8%rain/trainZ6.tfrecord 6.8%.9%us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 7.0%rain/trainZ6.tfrecord 7.3%rain/trainZ6.tfrecord 7.3%.4%rain/trainZ6.tfrecord 7.5%rain/trainZ6.tfrecord 7.5%us.data.yt8m.org/2/frame/train/trainZ6.tfrecord 7.8%rain/trainZ6.tfrecord 7.9%"
     ]
    }
   ],
   "source": [
    "# download the 1/100th of the training frame level data\n",
    "!curl data.yt8m.org/download.py | shard=1,100 partition=2/frame/train mirror=us python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the 1/100th of the validate frame level data\n",
    "%%capture # stops from displaying the output to manage file size\n",
    "curl data.yt8m.org/download.py | shard=1,20 partition=2/frame/validate mirror=us python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the 1/100th of the test frame level data\n",
    "%%capture # stops from displaying the output to manage file size\n",
    "curl data.yt8m.org/download.py | shard=1,20 partition=2/frame/test mirror=us python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract relevant frames from the videos to build a balanced dataset of indoor and outdoor images. The dataset should contain a few thousand images in total. This task can be performed with tools like OpenCV or FFmpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data on the labels for videos\n",
    "label_file = project_dir.joinpath('data/vocabulary.csv')\n",
    "# read the csv that contains infromation about labels of videos into dataframe\n",
    "df_labels = pd.read_csv(label_file.as_posix(), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(file):\n",
    "    '''\n",
    "    This function reads the frame level data of one tfrecord file\n",
    "    It goes through all the frames in the video and returns a three lists where each row\n",
    "    is an image (a frame from the video) and the column corresponds to the rgb data for that frame\n",
    "    It also extract the video ID and associated labels\n",
    "    \n",
    "    features: a list of features we want to extract from the tfrecord file\n",
    "    file: the path to a tfrecord file\n",
    "    '''\n",
    "    \n",
    "    # create an empty dataframe where the columns correspend to \n",
    "    # features we will extract\n",
    "    df = pd.DataFrame(columns = ['id', 'rgb', 'labels'])\n",
    "    \n",
    "    num_video = 1\n",
    "    for e in tf.python_io.tf_record_iterator(file): \n",
    "        print(num_video, len(df))\n",
    "        \n",
    "        tf_seq_example = tf.train.SequenceExample.FromString(e)\n",
    "        # get the number of frames in the video\n",
    "        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
    "        \n",
    "        # start interactive TF session\n",
    "        sess = tf.InteractiveSession()\n",
    "    \n",
    "        # iterate through frames\n",
    "        for i in range(n_frames):\n",
    "            # get the id of the video\n",
    "            video_id = tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.context.feature['id'].bytes_list.value[0],tf.uint8\n",
    "                ),tf.float32).eval()\n",
    "            # get rgb values for the frame image\n",
    "            # this returns an array of 1024 rgb elements for the image\n",
    "            arr_rgb = tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8\n",
    "                ),tf.float32).eval()  \n",
    "            # get the associated labels for the frame image\n",
    "            arr_labels = tf_seq_example.context.feature['labels'].int64_list.value\n",
    "            # add this list to the overall dataframe\n",
    "            \n",
    "            # create a row of the extracted information\n",
    "            row = {\n",
    "                'id': video_id,\n",
    "                'rgb': arr_rgb,\n",
    "                'labels': arr_labels\n",
    "            }\n",
    "            df = df.append(row, ignore_index=True)        \n",
    "        \n",
    "        sess.close()\n",
    "        num_video += 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the tensor flow files that we are going to read \n",
    "tf_files = [x for x in data_dir.glob('*.tfrecord')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "2 300\n",
      "3 522\n",
      "4 685\n",
      "5 820\n"
     ]
    }
   ],
   "source": [
    "# go through each of the tensor files\n",
    "# each tensor files contains thousands of videos\n",
    "# extract information about each the frames in the videos\n",
    "for file in tf_files[:1]:\n",
    "    df = extract_data(file.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train/test split of the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

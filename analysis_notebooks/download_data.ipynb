{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Create labeled image dataset with two classes\n",
    "\n",
    "1. Indoor photographs (e.g. Bedrooms, Bathrooms, Classrooms, Offices) \n",
    "2. Outdoor photographs (e.g. Landscapes, Skyscrapers, Mountains, Beaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/administrator/miniconda3/envs/dsst_env/lib/python3.6/site-packages/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import sklearn\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/administrator/Documents/pex_challenge/analysis_notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# project directory\n",
    "project_dir = Path('/Users/administrator/Documents/pex_challenge/')\n",
    "data_dir = project_dir.joinpath('data/yt8m/frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Download a subset of examples from the YouTube-8M labeled video dataset: https://research.google.com/youtube8m/explore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/administrator/Documents/pex_challenge/data/yt8m/frame\n"
     ]
    }
   ],
   "source": [
    "# change directories into data_dir, where we want to download the data\n",
    "%cd {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the 1/100th of the training frame level data\n",
    "!curl data.yt8m.org/download.py | shard=1,100 partition=2/frame/train mirror=us python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the 1/100th of the validate frame level data\n",
    "%%capture # stops from displaying the output to manage file size\n",
    "curl data.yt8m.org/download.py | shard=1,20 partition=2/frame/validate mirror=us python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the 1/100th of the test frame level data\n",
    "%%capture # stops from displaying the output to manage file size\n",
    "curl data.yt8m.org/download.py | shard=1,20 partition=2/frame/test mirror=us python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract relevant frames from the videos to build a balanced dataset of indoor and outdoor images. The dataset should contain a few thousand images in total. This task can be performed with tools like OpenCV or FFmpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data on the labels for videos\n",
    "label_file = project_dir.joinpath('data/vocabulary.csv')\n",
    "label_weight_file = project_dir.joinpath('data/vocabulary_with_weights.csv')\n",
    "# read the csv that contains infromation about labels of videos into dataframe\n",
    "df_labels = pd.read_csv(label_file.as_posix(), sep=',')\n",
    "df_labels_weights = pd.read_csv(label_weight_file.as_posix(), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(file, file_name = 'data'):\n",
    "    '''\n",
    "    This function reads the frame level data of one tfrecord file\n",
    "    It goes through all the frames in the video and returns a three lists where each row\n",
    "    is an image (a frame from the video) and the column corresponds to the rgb data for that frame\n",
    "    It also extract the video ID and associated labels\n",
    "    \n",
    "    file: the path to a tfrecord file\n",
    "    '''\n",
    "    \n",
    "    # create an empty dataframe where the columns correspend to \n",
    "    # features we will extract\n",
    "    df = pd.DataFrame(columns = ['id', 'rgb', 'labels'])\n",
    "    \n",
    "    num_video = 1\n",
    "    for e in tf.python_io.tf_record_iterator(file): \n",
    "        print(num_video, len(df))\n",
    "        \n",
    "        tf_seq_example = tf.train.SequenceExample.FromString(e)\n",
    "        # get the number of frames in the video\n",
    "        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
    "        \n",
    "        # start interactive TF session\n",
    "        sess = tf.InteractiveSession()\n",
    "    \n",
    "        # iterate through frames\n",
    "        for i in range(n_frames):\n",
    "            # get the id of the video\n",
    "            video_id = tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.context.feature['id'].bytes_list.value[0],tf.uint8\n",
    "                ),tf.float32).eval()\n",
    "            # get rgb values for the frame image\n",
    "            # this returns an array of 1024 rgb elements for the image\n",
    "            arr_rgb = tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8\n",
    "                ),tf.float32).eval()  \n",
    "            # get the associated labels for the frame image\n",
    "            arr_labels = tf_seq_example.context.feature['labels'].int64_list.value\n",
    "            # add this list to the overall dataframe\n",
    "            \n",
    "            # create a row of the extracted information\n",
    "            row = {\n",
    "                'id': video_id,\n",
    "                'rgb': arr_rgb,\n",
    "                'labels': arr_labels\n",
    "            }\n",
    "            df = df.append(row, ignore_index=True)        \n",
    "        \n",
    "        sess.close()\n",
    "        num_video += 1\n",
    "        \n",
    "        # save the dataframe after every video analyzed\n",
    "        df.to_csv(project_dir.joinpath('data/' + file_name + '.csv').as_posix())\n",
    "        \n",
    "        # if we have information of over 10,000 images\n",
    "        # break the for loop\n",
    "        if len(df) > 3000:\n",
    "            break\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_indoor_outdoor(df, vocabulary):\n",
    "    '''\n",
    "    This function takes in a dataframe holding information (id, rgb, labels) of\n",
    "    each different images. From the labels provided, it predicts whether this image\n",
    "    is indoor and outdoor and returns a dataframe with an additional dummy column 'indoor'.\n",
    "    If 'indoor' == 1, then the image is indoors. If 'indoor' == 0, then the image is outdoors.\n",
    "    \n",
    "    df: the dataframe with image information\n",
    "    vocabulary: a dataframe that maps numberical labels to strings (words)\n",
    "    '''\n",
    "    \n",
    "    # create an array that to hold information about whether an image is indoor\n",
    "    # or outdoor\n",
    "    indoor = []\n",
    "    \n",
    "    #iterate through the images\n",
    "    for row in df.iterrows():\n",
    "        # get the labels of the image\n",
    "        labels = row[1]['labels']\n",
    "        \n",
    "        weight_sum = 0\n",
    "        \n",
    "        #iterate through the labels\n",
    "        for label in labels:\n",
    "            # get the indoor weight for this label\n",
    "            weight_sum = weight_sum + vocabulary[vocabulary.Index == label].weights.values[0]\n",
    "        \n",
    "        # take the average of the weights\n",
    "        weight = weight_sum/len(labels)\n",
    "        # convert the weight to indoor 1 or 0 classified\n",
    "        if weight >= 0.5:\n",
    "            indoor.append(1)\n",
    "        else:\n",
    "            indoor.append(0)    \n",
    "        \n",
    "    df['indoor'] = indoor\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the tensor flow files that we are going to read \n",
    "tf_files = [x for x in data_dir.glob('*.tfrecord')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "2 229\n",
      "3 529\n",
      "4 749\n",
      "5 899\n",
      "6 1199\n"
     ]
    }
   ],
   "source": [
    "# go through each of the tensor files\n",
    "# each tensor files contains thousands of videos\n",
    "# extract information about each the frames in the videos\n",
    "for file in tf_files[1:2]:\n",
    "    df = extract_data(file.as_posix(), file_name = 'data1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = classify_indoor_outdoor(df, df_labels_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train/test split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat_data(df):\n",
    "    '''\n",
    "    This function takes in the dataframe as an input and converts it to a new dataframe\n",
    "    where the each value for the rgb has its own column\n",
    "    '''\n",
    "    \n",
    "    # define a new dataframe\n",
    "    df_new = pd.DataFrame([])\n",
    "    \n",
    "    num_row = 0\n",
    "    for row in df.iterrows():\n",
    "        # create a dictionary for the a new row of df_new\n",
    "        new_row = {}\n",
    "        new_row['id'] = row[1]['id']\n",
    "        \n",
    "        # get the rgb array of the image\n",
    "        arr_rgb = row[1]['rgb']\n",
    "        # iterate through the array\n",
    "        run = 0\n",
    "        for val in arr_rgb:\n",
    "            new_row['rgb' + str(run)] = val\n",
    "            run +=1\n",
    "        \n",
    "        new_row['indoor'] = row[1]['indoor']\n",
    "        \n",
    "        df_new = df_new.append(new_row, ignore_index=True)\n",
    "        num_row += 1\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reformat = reformat_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a train test split of the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reformat.iloc[:, 2:], df['indoor'], \n",
    "                                                                            test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the train/ test files\n",
    "X_train.to_csv(project_dir.joinpath('data/Xtrain.csv'))\n",
    "X_test.to_csv(project_dir.joinpath('data/Xtest.csv'))\n",
    "pd.DataFrame(y_train).to_csv(project_dir.joinpath('data/ytrain.csv'))\n",
    "pd.DataFrame(y_test).to_csv(project_dir.joinpath('data/ytest.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

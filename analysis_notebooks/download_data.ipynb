{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Create labeled image dataset with two classes\n",
    "\n",
    "1. Indoor photographs (e.g. Bedrooms, Bathrooms, Classrooms, Offices) \n",
    "2. Outdoor photographs (e.g. Landscapes, Skyscrapers, Mountains, Beaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import sklearn\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# project directory\n",
    "project_dir = Path('/Users/administrator/Documents/pex_challenge/')\n",
    "data_dir = project_dir.joinpath('data/yt8m/frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Download a subset of examples from the YouTube-8M labeled video dataset: https://research.google.com/youtube8m/explore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/administrator/Documents/pex_challenge/data/yt8m/frame\n"
     ]
    }
   ],
   "source": [
    "# change directories into data_dir, where we want to download the data\n",
    "%cd {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the 1/100th of the training frame level data\n",
    "!curl data.yt8m.org/download.py | shard=1,100 partition=2/frame/train mirror=us python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the 1/100th of the validate frame level data\n",
    "%%capture # stops from displaying the output to manage file size\n",
    "curl data.yt8m.org/download.py | shard=1,20 partition=2/frame/validate mirror=us python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the 1/100th of the test frame level data\n",
    "%%capture # stops from displaying the output to manage file size\n",
    "curl data.yt8m.org/download.py | shard=1,20 partition=2/frame/test mirror=us python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract relevant frames from the videos to build a balanced dataset of indoor and outdoor images. The dataset should contain a few thousand images in total. This task can be performed with tools like OpenCV or FFmpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data on the labels for videos\n",
    "label_file = project_dir.joinpath('data/vocabulary.csv')\n",
    "# read the csv that contains infromation about labels of videos into dataframe\n",
    "df_labels = pd.read_csv(label_file.as_posix(), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(file):\n",
    "    '''\n",
    "    This function reads the frame level data of one tfrecord file\n",
    "    It goes through all the frames in the video and returns a three lists where each row\n",
    "    is an image (a frame from the video) and the column corresponds to the rgb data for that frame\n",
    "    It also extract the video ID and associated labels\n",
    "    \n",
    "    file: the path to a tfrecord file\n",
    "    '''\n",
    "    \n",
    "    # create an empty dataframe where the columns correspend to \n",
    "    # features we will extract\n",
    "    df = pd.DataFrame(columns = ['id', 'rgb', 'labels'])\n",
    "    \n",
    "    num_video = 1\n",
    "    for e in tf.python_io.tf_record_iterator(file): \n",
    "        print(num_video, len(df))\n",
    "        \n",
    "        tf_seq_example = tf.train.SequenceExample.FromString(e)\n",
    "        # get the number of frames in the video\n",
    "        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
    "        \n",
    "        # start interactive TF session\n",
    "        sess = tf.InteractiveSession()\n",
    "    \n",
    "        # iterate through frames\n",
    "        for i in range(n_frames):\n",
    "            # get the id of the video\n",
    "            video_id = tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.context.feature['id'].bytes_list.value[0],tf.uint8\n",
    "                ),tf.float32).eval()\n",
    "            # get rgb values for the frame image\n",
    "            # this returns an array of 1024 rgb elements for the image\n",
    "            arr_rgb = tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8\n",
    "                ),tf.float32).eval()  \n",
    "            # get the associated labels for the frame image\n",
    "            arr_labels = tf_seq_example.context.feature['labels'].int64_list.value\n",
    "            # add this list to the overall dataframe\n",
    "            \n",
    "            # create a row of the extracted information\n",
    "            row = {\n",
    "                'id': video_id,\n",
    "                'rgb': arr_rgb,\n",
    "                'labels': arr_labels\n",
    "            }\n",
    "            df = df.append(row, ignore_index=True)        \n",
    "        \n",
    "        sess.close()\n",
    "        num_video += 1\n",
    "        \n",
    "        # if we have information of over 10,000 images\n",
    "        # break the for loop\n",
    "        if len(df) > 10000:\n",
    "            break\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_indoor_outdoor(df, vocabulary):\n",
    "    '''\n",
    "    This function takes in a dataframe holding information (id, rgb, labels) of\n",
    "    each different images. From the labels provided, it predicts whether this image\n",
    "    is indoor and outdoor and returns a dataframe with an additional dummy column 'indoor'.\n",
    "    If 'indoor' == 1, then the image is indoors. If 'indoor' == 0, then the image is outdoors.\n",
    "    \n",
    "    df: the dataframe with image information\n",
    "    vocabulary: a dataframe that maps numberical labels to strings (words)\n",
    "    '''\n",
    "    \n",
    "    # create an array that to hold information about whether an image is indoor\n",
    "    # or outdoor\n",
    "    indoor = []\n",
    "    \n",
    "    #iterate through the images\n",
    "    for row in df.iterrows():\n",
    "        converted_labels = []\n",
    "        \n",
    "        # get the labels of the image\n",
    "        labels = row.labels\n",
    "        \n",
    "        #iterate through the labels\n",
    "        for label in labels:\n",
    "            # convert the numberical label into a string value\n",
    "            string_label = vocabulary[vocabulary.index == label].Name.values[0]\n",
    "            converted_labels.append(string_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all the tensor flow files that we are going to read \n",
    "tf_files = [x for x in data_dir.glob('*.tfrecord')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each of the tensor files\n",
    "# each tensor files contains thousands of videos\n",
    "# extract information about each the frames in the videos\n",
    "for file in tf_files[:1]:\n",
    "    df = extract_data(file.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train/test split of the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
